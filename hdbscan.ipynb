{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import hdbscan\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_merged_df.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **HDBSCAN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Identify regions with sparse development where extending the grid is inefficient but wind microgrids could be feasible.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since HDBSCAN is a density-based algorithm,the clustering approach effectively groups areas based on density and other characteristics.\n",
    "* These clusters help indicate areas where extending the main grid might be inefficient, making them candidates for decentralized energy solutions, like wind or solar microgrids.\n",
    "* This algorithm is valuable for identifying potential zones for alternative energy solutions based on current density and infrastructure access patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'Income_Distribution' if it's categorical\n",
    "df = pd.get_dummies(df, columns=['Income_Distribution'], prefix='Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=250000\n",
    "m = 2500\n",
    "s = 5012\n",
    "e = 178934\n",
    "j = 2500\n",
    "df_1=df.head(j)\n",
    "df_2 = df.tail(m)\n",
    "df_3=df.iloc[s:e]\n",
    "df_4=df.sample(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1.to_csv(\"../first_2500_rows.csv\")\n",
    "# df_2.to_csv(\"../last_2500_rows.cvs\")\n",
    "df_3.to_csv(\"middle_rows.csv\")\n",
    "df_4.to_csv(\"random_rows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The one-hot encoded columns starting with \"Income_\"should be included\n",
    "income_columns = [col for col in df.columns if col.startswith('Income_')]\n",
    "clustering_data = df[['Pop_Density_2020', 'Wind_Speed', 'Latitude', 'Longitude', 'Grid_Value'] + income_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data to bring all features to a similar scale\n",
    "scaler = StandardScaler()\n",
    "clustering_data_scaled = scaler.fit_transform(clustering_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce to a manageable number of components\n",
    "pca = PCA(n_components=2)  # Adjust to 2 components for efficient clustering\n",
    "clustering_data_reduced = pca.fit_transform(clustering_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PCA components back to the DataFrame\n",
    "df['PCA_Component_1'] = clustering_data_reduced[:, 0]\n",
    "df['PCA_Component_2'] = clustering_data_reduced[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN clustering with optimized parameters\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(metric='manhattan', min_samples=10, min_cluster_size=50)\n",
    "clusters = hdbscan_clusterer.fit_predict(clustering_data_reduced)\n",
    "stability_scores = hdbscan_clusterer.probabilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clusters and stability scores back to the original DataFrame\n",
    "df['Cluster'] = clusters\n",
    "df['Stability_Score'] = stability_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDBSCAN with optimized parameters\n",
    "# hdbscan_clusterer = hdbscan.HDBSCAN(min_samples=10, min_cluster_size=50)  # Adjust min_samples and min_cluster_size as needed\n",
    "# clusters = hdbscan_clusterer.fit_predict(clustering_data_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out noise points (-1 label in HDBSCAN) before calculating metrics\n",
    "clustered_data = clustering_data_reduced[clusters != -1]\n",
    "valid_clusters = clusters[clusters != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access cluster stability scores from HDBSCAN\n",
    "# stability_scores = hdbscan_clusterer.probabilities_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(set(valid_clusters)) > 1:  # Ensure there's more than one cluster for evaluation\n",
    "    db_index = davies_bouldin_score(clustered_data, valid_clusters)\n",
    "    ch_index = calinski_harabasz_score(clustered_data, valid_clusters)\n",
    "    print(\"Davies-Bouldin Index:\", db_index)\n",
    "    print(\"Calinski-Harabasz Index:\", ch_index)\n",
    "else:\n",
    "    print(\"Insufficient clusters for evaluation metrics\")\n",
    "\n",
    "# Print stability scores and cluster labels\n",
    "print(\"Cluster Labels:\", clusters)\n",
    "print(\"Cluster Stability Scores:\", stability_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(clustering_data_reduced[:, 0], clustering_data_reduced[:, 1], c=clusters, cmap='viridis', s=5)\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.title('HDBSCAN Clustering Results on Full Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the scaler, PCA, and HDBSCAN model\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(pca, 'pca.pkl')\n",
    "joblib.dump(hdbscan_clusterer, 'hdbscan_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
